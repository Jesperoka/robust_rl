"""This code was generated by ChatGPT-4o."""
import numpy as np
import matplotlib.pyplot as plt
# from scipy.spatial.distance import mahalanobis

# Define the function to be minimized
def f(x):
    return (10 * x[0]**2 + x[1]**2) + 3 * x[0] + 5 * x[1]

# Define the gradient of the function
def grad_f(x):
    return np.array([20 * x[0] + 3, 2*x[1] + 5])

# Gradient descent with Euclidean distance
def gradient_descent_euclidean(start, learning_rate, n_steps):
    x = start
    path = [x]
    for _ in range(n_steps):
        x = x - learning_rate * grad_f(x)
        path.append(x)
    return np.array(path)

# Gradient descent with Mahalanobis distance
def gradient_descent_mahalanobis(start, learning_rate, n_steps, covariance_matrix):
    x = start
    path = [x]
    inv_cov_matrix = np.linalg.inv(covariance_matrix)
    for _ in range(n_steps):
        grad = grad_f(x)
        step = learning_rate * np.dot(inv_cov_matrix, grad)
        x = x - step
        path.append(x)
    return np.array(path)

# Plotting function
def plot_contours_and_descent(paths, title):
    x = np.linspace(-10, 10, 400)
    y = np.linspace(-10, 10, 400)
    X, Y = np.meshgrid(x, y)
    Z = f([X, Y])

    plt.figure()
    plt.contour(X, Y, Z, levels=50)
    for path in paths:
        plt.plot(path[:, 0], path[:, 1], marker='o')
        for i in range(len(path) - 1):
            plt.arrow(path[i, 0], path[i, 1], path[i+1, 0] - path[i, 0], path[i+1, 1] - path[i, 1], head_width=0.1, head_length=0.1, fc='r', ec='r')
    # plt.title(title)
    plt.xlabel('$\\theta_0$')
    plt.ylabel('$\\theta_1$')
    plt.tight_layout()
    plt.show()

# Starting point, learning rate and number of steps
start = np.array([2.5, 2.5])
learning_rate = 0.095
n_steps = 15

# Define the covariance matrix for Mahalanobis distance
covariance_matrix = np.array([[10, 0], [0, 1]])

# Perform gradient descent using Euclidean distance
path_euclidean = gradient_descent_euclidean(start, learning_rate, n_steps)

# Perform gradient descent using Mahalanobis distance
path_mahalanobis = gradient_descent_mahalanobis(start, learning_rate, n_steps, covariance_matrix)

# Plot the results
plot_contours_and_descent([path_euclidean], "Gradient Descent with Euclidean Distance")
plot_contours_and_descent([path_mahalanobis], "Gradient Descent with Mahalanobis Distance")

